{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0420099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Assignment Notebook\n",
    "# This is a single-file Python notebook script intended to be run as a Jupyter notebook\n",
    "# It contains: data generation class, model class with flexible depth/width, training loop\n",
    "# using PyTorch DataLoader and Adam, visualizations for train/val sets similar to Figure 1,\n",
    "# experiments over depths and widths, parameter counts, and BCE loss explanation.\n",
    "\n",
    "# NOTE: The original assignment PDF is included at: /mnt/data/DL_Assignment_1_2025.pdf\n",
    "# (This path was provided in the conversation.)\n",
    "\n",
    "# --- Cell 1: Imports and helpers ---\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# for saving snapshots / movie\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01394807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "RSEED = 42\n",
    "random.seed(RSEED)\n",
    "np.random.seed(RSEED)\n",
    "torch.manual_seed(RSEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56692d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Noisy XOR Dataset class ---\n",
    "class NoisyXORDataset(Dataset):\n",
    "    \"\"\"Generates samples for the noisy-xor problem.\n",
    "    Samples (x1, x2) are drawn from four cluster centers (0/1,0/1) with gaussian noise.\n",
    "    y is the XOR of the cluster center bits.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples: int = 1000, s: float = 0.1, seed: int = None):\n",
    "        super().__init__()\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        # cluster centers\n",
    "        centers = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "        k = centers.shape[0]\n",
    "        # pick centers uniformly\n",
    "        indices = np.random.randint(0, k, size=n_samples)\n",
    "        samples = centers[indices] + s * np.random.randn(n_samples, 2)\n",
    "        labels = (centers[indices][:,0].astype(int) ^ centers[indices][:,1].astype(int)).astype(np.float32)\n",
    "        self.x = torch.tensor(samples, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# quick plot function like Figure 1\n",
    "def plot_dataset(dataset: NoisyXORDataset, ax=None, title=None):\n",
    "    x = dataset.x.numpy()\n",
    "    y = dataset.y.numpy().squeeze()\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "    ax.scatter(x[y==0,0], x[y==0,1], label='Class 0', alpha=0.6)\n",
    "    ax.scatter(x[y==1,0], x[y==1,1], label='Class 1', alpha=0.6)\n",
    "    ax.set_xlim(-0.5, 1.5)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Flexible feed-forward network class ---\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, in_features:int=2, out_features:int=1, hidden_layers:List[int]=[3], activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_features\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(activation())\n",
    "            last = h\n",
    "        # final linear layer (output), no activation here: we'll use BCEWithLogitsLoss\n",
    "        layers.append(nn.Linear(last, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# helper to count parameters\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc05a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Training & evaluation utilities ---\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device=DEVICE):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device=DEVICE):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# decision boundary plotting\n",
    "def plot_decision_boundary(model, ax=None, title=None, device=DEVICE, resolution=200):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "    xx = np.linspace(-0.5, 1.5, resolution)\n",
    "    yy = np.linspace(-0.5, 1.5, resolution)\n",
    "    grid = np.stack(np.meshgrid(xx, yy), axis=-1).reshape(-1,2)\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        logits = model(torch.tensor(grid, dtype=torch.float32, device=device)).cpu().numpy().reshape(resolution, resolution)\n",
    "        probs = 1 / (1 + np.exp(-logits))\n",
    "    cs = ax.contourf(xx, yy, probs, levels=50, cmap=cm.RdBu, alpha=0.6)\n",
    "    ax.contour(xx, yy, probs, levels=[0.5], colors='k')\n",
    "    ax.set_xlim(-0.5, 1.5)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa20902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Cell 5: Experiment runner for depths/widths ---\n",
    "\n",
    "def run_experiment(seed:int=RSEED, s:float=0.1, n_samples_train:int=1000, n_samples_val:int=500,\n",
    "                   batch_size:int=64, lr:float=1e-3, epochs:int=200,\n",
    "                   depths:List[int]=[0,1,2,3], widths:List[int]=[1,2,3], runs_per_setting:int=5):\n",
    "    results = []\n",
    "    for depth in depths:\n",
    "        for width in widths:\n",
    "            setting_losses = []\n",
    "            setting_accs = []\n",
    "            param_counts = []\n",
    "            for run in range(runs_per_setting):\n",
    "                rs = seed + run\n",
    "                # generate datasets\n",
    "                train_ds = NoisyXORDataset(n_samples=n_samples_train, s=s, seed=rs)\n",
    "                val_ds = NoisyXORDataset(n_samples=n_samples_val, s=s, seed=rs+1000)\n",
    "                train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "                # build architecture: for depth=0 means no hidden layers -> direct linear model\n",
    "                if depth == 0:\n",
    "                    hidden = []\n",
    "                else:\n",
    "                    hidden = [width] * depth\n",
    "                model = FeedForwardNet(in_features=2, out_features=1, hidden_layers=hidden).to(DEVICE)\n",
    "                param_counts.append(count_parameters(model))\n",
    "                # use BCEWithLogitsLoss for numerical stability\n",
    "                criterion = nn.BCEWithLogitsLoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                # train for given epochs\n",
    "                for epoch in range(epochs):\n",
    "                    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device=DEVICE)\n",
    "                val_loss, val_acc = evaluate(model, val_loader, criterion, device=DEVICE)\n",
    "                setting_losses.append(val_loss)\n",
    "                setting_accs.append(val_acc)\n",
    "            results.append({\n",
    "                'depth': depth,\n",
    "                'width': width,\n",
    "                'mean_val_loss': float(np.mean(setting_losses)),\n",
    "                'std_val_loss': float(np.std(setting_losses)),\n",
    "                'mean_val_acc': float(np.mean(setting_accs)),\n",
    "                'std_val_acc': float(np.std(setting_accs)),\n",
    "                'param_counts': int(np.mean(param_counts))\n",
    "            })\n",
    "            print(f\"Depth {depth} Width {width}: mean loss {np.mean(setting_losses):.4f} Â± {np.std(setting_losses):.4f}, mean acc {np.mean(setting_accs):.3f}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c807a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Run a small, quick experiment (this is adjustable) ---\n",
    "if __name__ == '__main__':\n",
    "    # WARNING: full experiments can be compute heavy. The defaults below are modest.\n",
    "    results = run_experiment(seed=RSEED, s=0.1, n_samples_train=500, n_samples_val=300,\n",
    "                             batch_size=64, lr=1e-3, epochs=120,\n",
    "                             depths=[0,1,2,3], widths=[1,2,3], runs_per_setting=3)\n",
    "    # print summary\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "    print('\\nSummary of experiments:')\n",
    "    print(df)\n",
    "    # save to csv\n",
    "    df.to_csv('experiment_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973623fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Train minimal XOR network and save snapshots for decision boundary movie ---\n",
    "# We'll train 1 hidden layer with 2 units (tanh) and save snapshots over epochs.\n",
    "\n",
    "\n",
    "def train_and_snapshot(hidden_layers=[2], s=0.1, n_samples_train=500, n_samples_val=300,\n",
    "                       batch_size=64, lr=1e-3, epochs=200, snapshot_freq=10, out_dir='snapshots'):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    train_ds = NoisyXORDataset(n_samples=n_samples_train, s=s, seed=0)\n",
    "    val_ds = NoisyXORDataset(n_samples=n_samples_val, s=s, seed=100)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    model = FeedForwardNet(in_features=2, out_features=1, hidden_layers=hidden_layers).to(DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    snapshots = []\n",
    "    for epoch in range(epochs+1):\n",
    "        if epoch > 0:\n",
    "            train_one_epoch(model, train_loader, criterion, optimizer, device=DEVICE)\n",
    "        if epoch % snapshot_freq == 0:\n",
    "            fig, ax = plt.subplots(figsize=(4,4))\n",
    "            plot_decision_boundary(model, ax=ax, title=f'Epoch {epoch}')\n",
    "            plot_dataset(train_ds, ax=ax)  # overlay training points\n",
    "            fname = os.path.join(out_dir, f'snapshot_{epoch:04d}.png')\n",
    "            fig.savefig(fname)\n",
    "            plt.close(fig)\n",
    "            snapshots.append(fname)\n",
    "            print('Saved', fname)\n",
    "    # create gif\n",
    "    frames = [imageio.imread(fn) for fn in snapshots]\n",
    "    gif_path = os.path.join(out_dir, 'decision_evolution.gif')\n",
    "    imageio.mimsave(gif_path, frames, fps=2)\n",
    "    print('Saved gif to', gif_path)\n",
    "    return model, snapshots, gif_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running interactively, uncomment the following to run snapshot saving (it may take time):\n",
    "# model, snaps, gif = train_and_snapshot(hidden_layers=[2], epochs=200, snapshot_freq=10)\n",
    "\n",
    "\n",
    "# --- Cell 8: Discussion / Answer to subquestion (a) ---\n",
    "# (a) There are two versions of the binary cross entropy loss function in PyTorch.\n",
    "#\n",
    "# 1) torch.nn.BCELoss\n",
    "#    - This expects probabilities (i.e., the output should already have a sigmoid applied).\n",
    "#    - Numerically less stable when used in combination with a separate sigmoid layer, because\n",
    "#      the sigmoid and log operations can cause low-precision issues for extreme logits.\n",
    "#\n",
    "# 2) torch.nn.BCEWithLogitsLoss\n",
    "#    - This combines a sigmoid layer and the binary cross entropy loss in a single class.\n",
    "#    - It takes raw logits (no sigmoid activation on the model output) and applies a\n",
    "#      numerically stable formulation that avoids precision problems.\n",
    "#\n",
    "# Recommendation: Use BCEWithLogitsLoss when your network outputs raw logits (recommended).\n",
    "# If you intentionally output probabilities (after sigmoid), use BCELoss, but beware of\n",
    "# numerical stability for very large/small logits.\n",
    "\n",
    "\n",
    "# --- Cell 9: Notes and reproducibility ---\n",
    "# - The notebook is written to be modular: adjust the run_experiment parameters for larger sweeps.\n",
    "# - The 'train_and_snapshot' function specifically implements the request to save weights/snapshots\n",
    "#   for a minimal XOR network (1 hidden layer, 2 units) and produce a movie of the decision boundary.\n",
    "# - The experiments compute mean/std for loss and accuracy across multiple runs per setting.\n",
    "# - The code uses BCEWithLogitsLoss for stability (see discussion above).\n",
    "\n",
    "\n",
    "# Save this script as a Jupyter notebook or run the cells interactively.\n",
    "\n",
    "\n",
    "print('\\nNotebook script loaded. To execute the experiments, run this file as a notebook and use the functions provided.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
